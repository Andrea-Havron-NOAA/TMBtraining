---
title:  "A review of statistical computing with TMB"
author: "Andrea Havron<br>NOAA Fisheries, OST"
output:
  xaringan::moon_reader:
    self_contained: true
    css: ["xaringan-themer.css", "slides-style.css"]
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc( (%current% - 1) / (%total% - 1) * 100%);">
          </div>
        </div>`
---
layout: true

.footnote[U.S. Department of Commerce | National Oceanic and Atmospheric Administration | National Marine Fisheries Service]


<style type="text/css">

code.cpp{
  font-size: 14px;
}
code.r{
  font-size: 14px;
}


</style>


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(mvtnorm)
library(plotly)
#remotes::install_github("AckerDWM/gg3D")
library(gg3D)

```

```{r xaringan-tile-view, echo=FALSE}
# this gives you a tile navigation if you type "O" at any time
xaringanExtra::use_tile_view()
```
---
# Statistical Computing Review
<br>

```{r, echo = FALSE, out.width="60%", fig.align="left"}
knitr::include_graphics("static/TMB-trifecta.png")
```


---
# What is AD?

.three-column-left[
**Automatic Differentiation**<br>
Derivatives calculated automatically using the chain rule<br>
.p[
- Efficient: forward mode, O(n); reverse mode, O(m)
- Accurate 
- Higher order derivatives: easy
]]
.three-column-left[
**Symbolic Differentiation**<br>
Computer program converts function into exact derivative function<br>
.p[
- Inefficient: O(2n) for n input variables
- Exact 
- Higher order derivatives: difficult due to complexity
]]
.three-column-left[
**Numerical Differentiation**<br>
Approximation that relies on finite differences
.p[
- Efficient: O(n) 
- Trade-off between truncation error versus round-off error
- Higher order derivatives calculation difficult due to error accumulation
]]

---
# Computational Graph (Tape)
<br>
.three-column-left[
```{Rcpp, eval = FALSE}
//Program
v1: x = ?
v2: y = ?
v3: a = x * y
v4: b = sin(y)
v5: z = a + b
```
]
.three-column[
```{r, echo = FALSE, out.width="100%", fig.align="left"}
knitr::include_graphics("static/comp-graph.png")
```
]
.three-column-left[
```{Rcpp, eval = FALSE}
//Reverse Mode
dz = ?
da = dz
db = dz
dx = yda
dy = xda + cos(y)db
```
]

---
# Reverse Mode
.pull-left[
**Static (TMB: CppAD, TMBad)**<br>
The graph is constructed once before execution
.p[
- Less flexibility with conditional statements that depend on parameters. 
- Atomic functions can be used when conditional statements depend on parameters
- High portability 
- Graph optimization possible
]]

.pull-right[
**Dynamic (Stan: Stan Math Library, ADMB: AUTODIF)**<br>
The graph is defined as forward computation is executed at every iteration
.p[
- Flexibility with conditional statements
- Optimization routine implemented into executable
- Less time for graph optimization
]
]

---

class: middle

# Type Systems in R and C++
---
# Dynamic vs. Static Typing
<br>
.pull-left[
**R: Dynamic**
.p[
- Type checking occurs at run time
- The values and types associated with names can change
- Change in type tends to be implicit
]]

.pull-right[
**C++: Static**
.p[
- Type checking occurs at compile time
- The values associated with a given name can be limited to just a few types and may be unchangeable
- Change in type tends to be explicit
]]


---
# What is Templated C++?
<br><br>
* Generic programming
* Allows developer to write functions and classes that are independent of Type
* Templates are expanded at compile time

.pull-left[
```{Rcpp, eval = FALSE}

template <class T>
  T add(T x, T y){
  return x + y;
}

int main(){
  int a = 1;
  int b = 2;
  double c = 1.1;
  double d = 2.1;
  int d = add(a,b);
  double e = add(c,d);
}

```
]
.pull-right[
```{Rcpp, eval = FALSE}
int add(int x, int y){
  return x + y;
}
double add(double x, double y){
  return x + y;
}
```
]

---
# Setting up Templated C++
<br>
```{Rcpp, eval = FALSE}
template <class T>
T add(T x, T y){
  return x + y;
}
```
<br>
```{Rcpp, eval = FALSE}
template <typename Type>
Type add(Type x, Type y){
  return x + y;
}
```

---
# TMB AD Systems
<br>
.pull-left[
**CppAD**
- [CppAD package](https://coin-or.github.io/CppAD/doc/cppad.htm)
]
.pull-right[
**TMBad**<br>
- TMBad is available with TMB 1.8.0 and higher
]

---

class: middle

# Likelihood Review
---
# ML Inference

What is the likelihood for 30 successes in 100 trials?

.pull-left[
1. **Specify the model** <br><br>
$y ~ \sim Binomial(n, p)$
]
---
# ML Inference

What is the likelihood for 30 successes in 100 trials?

.pull-left[
1. Specify the model
2. **Calculate the likelihood**<br><br>
$L(p; n, y) = \frac{n!}{y!(n-y)!}p^y(1-p)^{n-y}$
]

.pull-right[
$L(p; n = 100, y = 30)$
```{r, eval=TRUE, out.width = '80%', echo = FALSE}
curve(dbinom(30,100,x),0,0.75, ylab = 'L(p)', xlab = 'p')
```
]
---

# ML Inference

What is the likelihood for 30 successes in 100 trials?

.pull-left[
1. Specify the model
2. Calculate the likelihood
3. **Calculate the negative log-likelihood**<br><br>
$-\ell(p; n, y) = -[ln\big(\frac{n!}{y!(n-y)!}\big) + yln(p)$<br>
     $$+ (n-y)ln(1-p)]$$
]

.pull-right[
$-ln\big[L(p; n = 100, y = 30)\big]$
```{r, eval=TRUE, out.width = '80%', echo = FALSE}
curve(-log(dbinom(30,100,x)),0,0.75, ylab = 'l(p)', xlab = 'p')
```
]
---

# ML Inference

What is the likelihood for 30 successes in 100 trials?

.pull-left[
1. Specify the model
2. Calculate the likelihood
3. Calculate the negative log-likelihood
$-\ell(p; n, y) = -[ln\big(\frac{n!}{y!(n-y)!}\big) + yln(p)$<br>
     $$+ (n-y)ln(1-p)]$$
4. **Calculate the derivative w.r.t. $p$**<br><br>
$\frac{d(\ell(p; n, y))}{dp} = \frac{y}{p}- \frac{n-y}{1-p}$
]

.pull-right[
$-ln\big[L(p; n = 100, y = 30)\big]$
```{r, eval=TRUE, out.width = '80%', echo = FALSE}
curve(-log(dbinom(30,100,x)),0,0.75, ylab = 'l(p)', xlab = 'p')
```
]
---

# ML Inference

What is the likelihood for 30 successes in 100 trials?

.pull-left[
1. Specify the model
2. Calculate the likelihood
3. Calculate the negative log-likelihood
4. Calculate the derivate wrt p
5. **Set to 0 and solve for MLE**<br>
$0 = \frac{y}{p}- \frac{n-y}{1-p}$ <br>
$E[p] = \frac{y}{n}$<br>
$E[y] = np$
]

.pull-right[
$-ln\big[L(p; N = 100, y = 30)\big]$
```{r, eval=TRUE, out.width = '70%', echo = FALSE}
curve(-log(dbinom(30,100,x)),0,0.75, ylab = 'l(p)', xlab = 'p')
nll <- -dbinom(30,100,.3,TRUE)
segments(0.1, nll, 0.5, nll, col='red', lwd=2)
```
<br>
$\hat{p} = \frac{30}{100} = 0.3$
]

---

# ML Inference

What is the likelihood for 30 successes in 100 trials?

.pull-left[
1. Specify the model
2. Calculate the likelihood
3. Calculate the negative log-likelihood
4. Calculate the derivate wrt p
5. Set to 0 and solve for MLE<br>
$0 = \frac{y}{p}- \frac{n-y}{1-p}$ <br>
$E[y] = np$
6. **Approximate Var[p] using the second derivative**<br>
$-\frac{y}{p^2} - \frac{(n-y)}{(1-p)^2}$<br>
$-\frac{np}{p^2} - \frac{(n-np)}{(1-p)^2}$<br>
$-\frac{n}{p} - \frac{n(1-p)}{1-p}$<br>
$l''(p) = -\frac{n(1-p)}{p}$<br>
$Var[p] = \frac{p}{n(1-p)}$
]

.pull-right[
$Var[p] \approx -\frac{1}{l''(p)}$<br>
$SE[p] \approx \sqrt{ \frac{.3}{100(.7)}} \approx 0.065$<br>
```{r, eval=TRUE, out.width = '70%', echo = FALSE}
curve(-log(dbinom(30,100,x)),0,0.75, ylab = 'l(p)', xlab = 'p')
nll <- -dbinom(30,100,.3,TRUE)
points(0.3, nll, col = 'red')
confint <- .3 + c(-1,1)*2*sqrt(.3/70)
segments(confint[1], -log(dbinom(30,100,confint[1])), confint[1], 100, col='red', lwd=2)
segments(confint[2], -log(dbinom(30,100,confint[2])), confint[2], 100, col='red', lwd=2)
text(.3, 20, "95% confint", col='red', cex = 2)
```
]
---
#Multivariate asymptotics

* For N-d models, the curvature is represented by a NxN **Hessian** matrix of 2nd partial derivatives
* Inverting the negative Hessian gives us a covariance matrix

\begin{align}
(\mathbb{H}_{f})_{i,j} &= \frac{\partial^2f}{\partial \theta_{i}, \partial x\theta_{j}} = \frac{-1}{Var(\Theta)}
\end{align}

.pull-left[
```{r mvnorm, echo=FALSE, warning = FALSE, message=FALSE}


    
     cor2cov <- function(R,sd2){
    S <- c(sd2, sd2)
    diag(S) %*% R %*% diag(S)
  }

  C <- cbind(c(1, .4), c(.4, 1))
  Sigma <- cor2cov(C, 1)
  # df <- as.data.frame(expand.grid(x = seq(-3,3,.5), y = seq(-3,3,.5)))
  # df$z <- 0
  # for(i in 1:nrow(df)){
  #   df$z[i] <-   dmvnorm(c(df$x[i], df$y[i]), sigma = Sigma)
  # }
  # df$z = apply(df, 1, function(t) dmvnorm(c(t[1], t[2]), sigma = Sigma))
  # for(i in 1:length(x.seq)){
  #     df[i,] <- dmvnorm(c(x.seq[i],y.seq[j]), sigma = Sigma)
  #   }
  # # }
  # ggplot(df, aes(x=x, y=x, z=z)) + 
  #   theme_void() + 
  #   gg3D::axes_3D()+
  # stat_wireframe(alpha=.5) +
  # stat_3D(aes(color=z), alpha=.5) +
  # theme_void() +
  # theme(legend.position = "none") +
  # scale_color_gradientn(colors=plot3D::jet2.col()) +
  # labs_3D(hjust=c(0,1,1), vjust=c(1, 1, -0.2), angle=c(0, 0, 90))
  
 # plot.ly(x=df$x, y =)
  x <- seq(-3, 3,  .5)
    y <- x
    z <- matrix(0,length(x),length(y))
    for(i in 1:length(x)){
      for(j in 1:length(y)){
        z[i,j] <- dmvnorm(c(x[i],y[j]), sigma = Sigma)
      }
    }
  persp(x, y, -log(z), zlim = range(0,12))
 # print(round(-solve(Sigma),3))

```
]

.pull-right[
[test](C:/Users/Andrea.Havron/Documents/github-noaa/Andrea-Havron-NOAA/TMBtraining/Training_Materials/Beginner_TMB/MVNplot.Rmd)
]
